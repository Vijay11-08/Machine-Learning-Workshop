{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNz7+aJntOWMQoZngrlG4XZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"phTAr0IRYvmg"},"outputs":[],"source":["#1.\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["#2.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"yT_PCG7oY-Xu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/AI-ML Workshop /Bengaluru_House_Data.csv')\n","df"],"metadata":{"id":"a9CEIMFaZBbW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"yGKz6lgubfIF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['balcony']=df['balcony'].fillna(df['balcony'].mean())\n","df.isna().sum()"],"metadata":{"id":"y7bv1YAFcjSX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.society.mode()"],"metadata":{"id":"t8O-aH19cu77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.fillna({'society':'GrrvaGr'},inplace=True)\n","df"],"metadata":{"id":"UhJb_8xX9R-8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.society.isna().sum()"],"metadata":{"id":"7Ppk1SEc-Cdm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.fillna({'society':'GrrvaGr'},inplace=True)"],"metadata":{"id":"Ez7tcy_s-IEg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the 'size' column into two columns: 'Size_Value' and 'Size_Unit'\n","df[['Size_Value', 'Size_Unit']] = df['size'].str.extract(r'(\\d+)\\s*(\\D+)')\n","\n","# Convert 'Size_Value' to integer\n","df['Size_Value'] = pd.to_numeric(df['Size_Value'], errors='coerce').astype('Int64')  # Using 'Int64' for integer handling with NaN\n","\n","# Display the updated DataFrame with the new columns\n","print(df[['size', 'Size_Value', 'Size_Unit']].head())\n","df"],"metadata":{"id":"K0sKkZpZ_FLy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numeric_columns = df.select_dtypes(include=['number'])\n","averages = numeric_columns.mean()\n","print(averages)"],"metadata":{"id":"8z1Zz9U7_GGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Step 2: Define parameters\n","m = 2  # Slope\n","c = 1  # Y-intercept\n","\n","# Step 3: Choose x-values\n","x_values = [1, 2, 3, 4, 5]  # Specific x-values\n","\n","# Step 4: Calculate y-values\n","y_values = [m * x + c for x in x_values]\n","\n","# Step 5: Create a bar chart\n","plt.bar(x_values, y_values, color='skyblue')\n","plt.xlabel('x - Independent Variable')\n","plt.ylabel('y - Dependent Variable')\n","plt.title('Bar Chart of y = mx + c')\n","plt.xticks(x_values)  # Ensure all x-values are shown on the x-axis\n","plt.grid(axis='y', linestyle='--')  # Add horizontal grid lines\n","plt.show()"],"metadata":{"id":"3LR4PS_uMGaG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Step 2: Define parameters\n","m = 2  # Slope\n","c = 1  # Y-intercept\n","\n","# Step 3: Generate x-values\n","x = np.linspace(0, 10, 100)  # 100 points between 0 and 10\n","\n","# Step 4: Calculate y-values\n","y = m * x + c\n","\n","# Step 5: Plot the graph\n","plt.plot(x, y, label=f'y = {m}x + {c}', color='blue', linewidth=2)\n","\n","# Step 6: Customize the chart\n","plt.xlabel('x - Independent Variable', fontsize=12)\n","plt.ylabel('y - Dependent Variable', fontsize=12)\n","plt.title('Line Graph of y = mx + c', fontsize=14)\n","plt.legend(fontsize=10)\n","plt.grid(True, linestyle='--', alpha=0.7)\n","plt.xlim([0, 10])  # Set the x-axis limits\n","plt.ylim([0, 25])  # Set the y-axis limits\n","plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5) # Add a horizontal line at y=0\n","plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5) # Add a vertical line at x=0\n","plt.show()"],"metadata":{"id":"OEgGoBFCAcam"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Step 2: Define data\n","x = np.array([1, 2, 3, 4, 5])\n","y = np.array([5, 7, 9, 11, 13])\n","\n","# Step 3: Calculate regression line\n","coefficients = np.polyfit(x, y, 1)  # 1 for linear (degree 1)\n","regression_line = np.poly1d(coefficients)\n","\n","# Step 4: generate x values for the line\n","x_line = x\n","\n","# Step 5: Calculate y values for the line\n","y_line = regression_line(x_line)\n","\n","# Step 6: Plot\n","plt.scatter(x, y, color='red', label='Data Points')  # Scatter plot of data points\n","plt.plot(x_line, y_line, color='blue', label=f'Regression Line: y={coefficients[0]:.2f}x+{coefficients[1]:.2f}')  # Regression line\n","\n","# Step 7: Customize\n","plt.xlabel('x - Independent Variable', fontsize=12)\n","plt.ylabel('y - Dependent Variable', fontsize=12)\n","plt.title('Linear Regression Chart', fontsize=14)\n","plt.legend(fontsize=10)\n","plt.grid(True, linestyle='--', alpha=0.7)\n","plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)  # Add a horizontal line at y=0\n","plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)  # Add a vertical line at x=0\n","plt.show()"],"metadata":{"id":"s8w6pZG4As4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","# Step 1: Generate Data\n","# Independent variables\n","X1 = np.array([1, 2, 3, 4, 5])\n","X2 = np.array([2, 4, 6, 8, 10])\n","X = np.column_stack((X1, X2))  # Combine X1 and X2\n","\n","# Dependent variable\n","Y = np.array([5, 7, 9, 11, 13])\n","\n","# Step 2: Fit the Model\n","model = LinearRegression()\n","model.fit(X, Y)\n","\n","# Get coefficients and intercept\n","b0 = model.intercept_\n","b1, b2 = model.coef_\n","print(f\"Intercept (b0): {b0}\")\n","print(f\"Coefficient for X1 (b1): {b1}\")\n","print(f\"Coefficient for X2 (b2): {b2}\")\n","\n","# Step 3: Predictions\n","Y_pred = model.predict(X)\n","\n","# Step 4: Evaluate the Model\n","print(f\"Mean Squared Error: {mean_squared_error(Y, Y_pred):.2f}\")\n","print(f\"R-squared: {r2_score(Y, Y_pred):.2f}\")\n","\n","# Step 5: Visualize Results in 3D\n","fig = plt.figure(figsize=(10, 7))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","# Plot original data points\n","ax.scatter(X1, X2, Y, color='red', label='Actual Data')\n","\n","# Plot regression plane\n","X1_grid, X2_grid = np.meshgrid(np.linspace(1, 5, 10), np.linspace(2, 10, 10))\n","Y_grid = b0 + b1 * X1_grid + b2 * X2_grid\n","ax.plot_surface(X1_grid, X2_grid, Y_grid, alpha=0.5, cmap='viridis')\n","\n","# Labels and Title\n","ax.set_xlabel('X1')\n","ax.set_ylabel('X2')\n","ax.set_zlabel('Y')\n","ax.set_title('Multiple Linear Regression')\n","ax.legend()\n","plt.show()\n"],"metadata":{"id":"e1qXFlNOBvn4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Generate data\n","y_true = 3\n","y_pred = np.linspace(0, 5, 100)\n","\n","# Calculate loss\n","mse = (y_pred - y_true)**2\n","mae = np.abs(y_pred - y_true)\n","\n","# Plot\n","plt.figure(figsize=(10, 5))\n","plt.plot(y_pred, mse, label='MSE (Mean Squared Error)', color='blue')\n","plt.plot(y_pred, mae, label='MAE (Mean Absolute Error)', color='red')\n","plt.xlabel('Predicted Value (y_pred)')\n","plt.ylabel('Loss')\n","plt.title('Comparison of MSE and MAE')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"4ZDEKoM0ENde"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.linear_model import LinearRegression\n","\n","# Given data\n","x = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Reshaping for sklearn\n","y = np.array([5, 7, 9, 11, 13])\n","\n","# Step 1: Fit a linear regression model\n","model = LinearRegression()\n","model.fit(x, y)\n","\n","# Predicted values\n","y_pred = model.predict(x)\n","\n","# Step 2: Calculate MSE manually\n","mse_manual = np.mean((y - y_pred)**2)\n","print(f\"Mean Squared Error (Manual): {mse_manual}\")\n","\n","# Step 3: Verify using sklearn\n","from sklearn.metrics import mean_squared_error\n","mse_sklearn = mean_squared_error(y, y_pred)\n","print(f\"Mean Squared Error (sklearn): {mse_sklearn}\")\n"],"metadata":{"id":"YnDtNjDtFNGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Given data\n","x = np.array([2, 3, 4, 5, 6])\n","y = np.array([10, 14, 18, 22, 26])  # Actual values\n","y_pred = np.array([8, 11, 14, 17, 20])  # Predicted values\n","\n","# Calculate squared errors\n","squared_errors = (y - y_pred)**2\n","mse = np.mean(squared_errors)\n","\n","# Plot the squared errors\n","plt.figure(figsize=(10, 6))\n","plt.plot(x, squared_errors, marker='o', label='Squared Errors', color='blue')\n","plt.axhline(y=mse, color='red', linestyle='--', label=f'MSE = {mse:.2f}')\n","plt.title('Loss Function Visualization (Squared Errors)')\n","plt.xlabel('x (Input Data)')\n","plt.ylabel('Squared Error')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"MyUn0YGXFelG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Given data\n","x = np.array([2, 3, 4, 5, 6])\n","y = np.array([10, 14, 18, 22, 26])  # Actual values\n","y_pred = np.array([8, 11, 14, 17, 20])  # Predicted values\n","\n","# Calculate squared errors\n","squared_errors = (y - y_pred)**2\n","mse = np.mean(squared_errors)\n","\n","# Plot the squared errors as discrete points\n","plt.figure(figsize=(10, 6))\n","plt.scatter(x, squared_errors, color='blue', label='Squared Errors', s=100)  # Markers for errors\n","plt.axhline(y=mse, color='red', linestyle='--', label=f'MSE = {mse:.2f}')  # MSE as reference\n","plt.title('Loss Function Visualization (Discrete Squared Errors)')\n","plt.xlabel('x (Input Data)')\n","plt.ylabel('Squared Error')\n","plt.legend()\n","plt.grid(True, linestyle='--', alpha=0.7)\n","plt.show()\n"],"metadata":{"id":"3Gq2-tVgF59P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Given data\n","x = np.array([2, 3, 4, 5, 6])\n","y = np.array([10, 14, 18, 22, 26])  # Actual values\n","y_pred = np.array([8, 11, 14, 17, 20])  # Predicted values\n","\n","# Calculate squared errors\n","squared_errors = (y - y_pred)**2\n","mse = np.mean(squared_errors)\n","\n","# Plot actual points\n","plt.figure(figsize=(12, 6))\n","plt.scatter(x, y, color='green', label='Actual Values (y)', s=100, marker='o')\n","\n","# Plot predicted points\n","plt.scatter(x, y_pred, color='orange', label='Predicted Values ($\\hat{y}$)', s=100, marker='x')\n","\n","# Plot squared errors\n","plt.scatter(x, squared_errors, color='blue', label='Squared Errors', s=100, marker='s')\n","\n","# MSE reference line\n","plt.axhline(y=mse, color='red', linestyle='--', label=f'MSE = {mse:.2f}')\n","\n","# Chart details\n","plt.title('Visualization of Actual Values, Predictions, and Errors', fontsize=14)\n","plt.xlabel('x (Input Data)', fontsize=12)\n","plt.ylabel('y / Errors', fontsize=12)\n","plt.legend(fontsize=12)\n","plt.grid(True, linestyle='--', alpha=0.7)\n","plt.show()\n"],"metadata":{"id":"Kb6xCD1hGSUw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Data\n","x = np.array([2, 3, 4, 5, 6])\n","y = np.array([10, 14, 18, 22, 26])\n","\n","# Hyperparameters\n","alpha = 0.01  # Learning rate\n","iterations = 1000\n","m = 0  # Initial slope\n","c = 0  # Initial intercept\n","\n","# Gradient Descent\n","n = len(x)\n","for _ in range(iterations):\n","    y_pred = m * x + c\n","    dm = -(2/n) * np.sum(x * (y - y_pred))  # Gradient for m\n","    dc = -(2/n) * np.sum(y - y_pred)       # Gradient for c\n","    m -= alpha * dm                        # Update m\n","    c -= alpha * dc                        # Update c\n","\n","print(f\"Slope (m): {m:.2f}, Intercept (c): {c:.2f}\")\n"],"metadata":{"id":"eUz-8z2gGgjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","# Sample data\n","x = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n","y = np.array([2, 6, 12, 20, 30])\n","\n","# Degrees to compare\n","degrees = [1, 2, 3]\n","\n","# Plot\n","plt.figure(figsize=(12, 6))\n","plt.scatter(x, y, color='red', label='Data Points')\n","\n","for degree in degrees:\n","    # Transform features to polynomial\n","    poly = PolynomialFeatures(degree=degree)\n","    x_poly = poly.fit_transform(x)\n","\n","    # Fit model\n","    model = LinearRegression()\n","    model.fit(x_poly, y)\n","    y_pred = model.predict(x_poly)\n","\n","    # Plot polynomial fit\n","    plt.plot(x, y_pred, label=f'Degree {degree} (MSE: {mean_squared_error(y, y_pred):.2f})')\n","\n","plt.title('Polynomial Regression with Different Degrees')\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"yI0K2BcXHsiT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.svm import SVC\n","from sklearn.datasets import make_blobs\n","\n","# Generate a sample dataset\n","X, y = make_blobs(n_samples=100, centers=2, random_state=6, cluster_std=1.5)\n","\n","# Create and fit the SVM model\n","model = SVC(kernel='linear', C=1)\n","model.fit(X, y)\n","\n","# Plot the data points\n","plt.figure(figsize=(10, 6))\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=50, edgecolors='k', label=\"Data Points\")\n","\n","# Get the separating hyperplane\n","w = model.coef_[0]\n","b = model.intercept_[0]\n","x_hyperplane = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n","y_hyperplane = -(w[0] * x_hyperplane + b) / w[1]\n","plt.plot(x_hyperplane, y_hyperplane, color='black', label=\"Hyperplane\")\n","\n","# Plot the margin lines\n","margin = 1 / np.sqrt(np.sum(w**2))\n","y_margin_upper = y_hyperplane + margin\n","y_margin_lower = y_hyperplane - margin\n","plt.plot(x_hyperplane, y_margin_upper, 'r--', label=\"Margin\")\n","plt.plot(x_hyperplane, y_margin_lower, 'r--')\n","\n","# Highlight support vectors\n","support_vectors = model.support_vectors_\n","plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=200, facecolors='none', edgecolors='k', label=\"Support Vectors\")\n","\n","# Plot customization\n","plt.title(\"SVM with Linear Kernel\", fontsize=14)\n","plt.xlabel(\"Feature 1\", fontsize=12)\n","plt.ylabel(\"Feature 2\", fontsize=12)\n","plt.legend(fontsize=12)\n","plt.grid(True, linestyle='--', alpha=0.7)\n","plt.show()\n"],"metadata":{"id":"o_BTF80vIJQT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","\n","# Load the Bengaluru house dataset\n","df = pd.read_csv('/content/drive/MyDrive/AI-ML Workshop /Bengaluru_House_Data.csv')\n","\n","# Display dataset information\n","print(\"Dataset shape:\", df.shape)\n","print(df.head())\n","\n","# Preprocess the data\n","# Convert categorical columns (e.g., location) to dummy variables\n","df = pd.get_dummies(df, drop_first=True)\n","\n","# Check for missing values\n","print(\"\\nMissing values:\")\n","print(df.isnull().sum())\n","\n","# Drop rows with missing target values\n","df = df.dropna(subset=['price'])\n","\n","# Define features (X) and target variable (y)\n","X = df.drop('price', axis=1)  # Features (excluding target column 'price')\n","y = df['price']  # Target (price)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train the Linear Regression model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(\"\\nModel Evaluation:\")\n","print(f\"Mean Squared Error (MSE): {mse}\")\n","print(f\"R^2 Score: {r2}\")\n","\n","# Plot actual vs predicted values\n","plt.figure(figsize=(8, 6))\n","plt.scatter(y_test, y_pred, alpha=0.7)\n","plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label=\"Ideal Fit\")\n","plt.xlabel(\"Actual Values\")\n","plt.ylabel(\"Predicted Values\")\n","plt.title(\"Actual vs Predicted House Prices\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"nQdrIES3Ies6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","\n","# Load the Bengaluru house dataset\n","df = pd.read_csv('/content/drive/MyDrive/AI-ML Workshop /Bengaluru_House_Data.csv')\n","\n","# Display dataset information\n","print(\"Dataset shape:\", df.shape)\n","print(df.head())\n","\n","# Check for missing values in the dataset\n","print(\"\\nMissing values before filling:\")\n","print(df.isnull().sum())\n","\n","# Handle missing values:\n","# For categorical columns (e.g., 'location', 'area_type', 'availability', 'society'), use mode\n","df['location'] = df['location'].fillna(df['location'].mode()[0])\n","df['area_type'] = df['area_type'].fillna(df['area_type'].mode()[0])\n","df['availability'] = df['availability'].fillna(df['availability'].mode()[0])\n","df['society'] = df['society'].fillna(df['society'].mode()[0])\n","\n","# For numerical columns, use median\n","df['size'] = df['size'].fillna(df['size'].mode()[0])  # 'size' seems categorical but has missing values, so we fill with mode\n","df['bath'] = df['bath'].fillna(df['bath'].median())\n","df['balcony'] = df['balcony'].fillna(df['balcony'].median())\n","df['total_sqft'] = df['total_sqft'].fillna(df['total_sqft'].median())\n","\n","# Check for missing values after filling\n","print(\"\\nMissing values after filling:\")\n","print(df.isnull().sum())\n","\n","# Convert categorical columns to dummy variables\n","df = pd.get_dummies(df, drop_first=True)\n","\n","# Define features (X) and target variable (y)\n","X = df.drop('price', axis=1)  # Features (excluding target column 'price')\n","y = df['price']  # Target (price)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train the Linear Regression model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(\"\\nModel Evaluation:\")\n","print(f\"Mean Squared Error (MSE): {mse}\")\n","print(f\"R^2 Score: {r2}\")\n","\n","# Plot actual vs predicted values\n","plt.figure(figsize=(8, 6))\n","plt.scatter(y_test, y_pred, alpha=0.7)\n","plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label=\"Ideal Fit\")\n","plt.xlabel(\"Actual Values\")\n","plt.ylabel(\"Predicted Values\")\n","plt.title(\"Actual vs Predicted House Prices\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"FxRst72oSWx-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC  # Support Vector Machine Classifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","\n","# Load a sample dataset (Iris dataset as an example)\n","from sklearn.datasets import load_iris\n","data = load_iris()\n","\n","# Convert it to a DataFrame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target  # Add target variable to the dataframe\n","\n","# Display the first few rows of the dataset\n","print(df.head())\n","\n","# Define features (X) and target (y)\n","X = df.drop('target', axis=1)  # Features (excluding target column 'target')\n","y = df['target']  # Target variable\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train the Support Vector Machine Classifier\n","svm_model = SVC(kernel='linear')  # Using a linear kernel for simplicity\n","svm_model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = svm_model.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Print classification report and confusion matrix\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n","print(\"\\nConfusion Matrix:\")\n","print(confusion_matrix(y_test, y_pred))\n","\n","# Plot confusion matrix\n","import seaborn as sns\n","cm = confusion_matrix(y_test, y_pred)\n","plt.figure(figsize=(6, 4))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")\n","plt.show()\n"],"metadata":{"id":"3eUn7ruLVIpf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.svm import SVC\n","from sklearn.datasets import make_classification\n","\n","# Create a simple 2D dataset with two classes\n","X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_clusters_per_class=1, random_state=42)\n","\n","# Train a Support Vector Machine Classifier\n","svm_model = SVC(kernel='linear')  # Use a linear kernel\n","svm_model.fit(X, y)\n","\n","# Plot the points (data)\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', marker='o', edgecolor='k', s=100)\n","\n","# Plot the hyperplane (decision boundary)\n","ax = plt.gca()\n","xlim = ax.get_xlim()\n","ylim = ax.get_ylim()\n","\n","# Create a grid to plot the decision boundary\n","xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))\n","Z = svm_model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","\n","# Plot decision boundary and margins\n","plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n","\n","# Highlight support vectors\n","plt.scatter(svm_model.support_vectors_[:, 0], svm_model.support_vectors_[:, 1], s=200, facecolors='none', edgecolors='black', linewidth=2, label=\"Support Vectors\")\n","\n","# Add labels and title\n","plt.title(\"SVM Hyperplane and Support Vectors (2D Example)\")\n","plt.xlabel(\"Feature 1\")\n","plt.ylabel(\"Feature 2\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"hCbr7nGVWRzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","\n","# Load a sample dataset (e.g., Iris dataset)\n","iris = datasets.load_iris()\n","X = iris.data[:, :2]  # Taking the first two features for visualization\n","y = iris.target\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create the SVM model with a linear kernel\n","svm_model = SVC(kernel='linear')\n","\n","# Train the model\n","svm_model.fit(X_train, y_train)\n","\n","# Create a grid to plot decision boundaries\n","xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 500),\n","                     np.linspace(X[:, 1].min(), X[:, 1].max(), 500))\n","\n","# Get the decision function\n","Z = svm_model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","\n","# Plot the decision boundary and margin\n","plt.contour(xx, yy, Z, levels=[-1, 0, 1], cmap=\"coolwarm\", alpha=0.75)\n","plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, cmap=\"autumn\", marker='o')\n","plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, cmap=\"winter\", marker='x')\n","plt.title(\"SVM with Linear Kernel and Hyperplane\")\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.show()\n"],"metadata":{"id":"fjCavTgaWnm5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","from sklearn.svm import SVC\n","\n","# Generate a dataset\n","X, y = datasets.make_classification(n_samples=100, n_features=2, n_classes=2, n_clusters_per_class=1, n_redundant=0, random_state=42)\n","\n","# Train an SVM classifier\n","clf = SVC(kernel='linear')  # Linear SVM\n","clf.fit(X, y)\n","\n","# Plotting the decision boundary\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', marker='o')\n","plt.title('SVM in 2D (Linear)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","\n","# Plot the decision boundary\n","ax = plt.gca()\n","xlim = ax.get_xlim()\n","ylim = ax.get_ylim()\n","xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n","Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n","\n","plt.show()\n"],"metadata":{"id":"5UBPqwjyXPdG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hXpgFxx8XchA"},"execution_count":null,"outputs":[]}]}